# Projet de Comparaison de ModÃ¨les pour le Cancer du Sein

## ğŸ“‹ Vue d'ensemble
Ce projet implÃ©mente un pipeline complet d'apprentissage automatique pour la dÃ©tection du cancer du sein, comparant diffÃ©rents algorithmes de classification en utilisant le dataset Wisconsin Breast Cancer. Le projet inclut l'entraÃ®nement des modÃ¨les, les tests, le suivi des performances avec MLflow, et un tableau de bord Streamlit pour visualiser les rÃ©sultats.

![Dashboard Interactif](assets/images/accueil.png)

## ğŸ—ï¸ Structure du Projet
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ dashboard
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ run_dashboard.py
â”œâ”€â”€ data
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ models_training.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test.py
```

## ğŸš€ FonctionnalitÃ©s
- EntraÃ®nement et comparaison de plusieurs modÃ¨les de classification :
  - RÃ©gression Logistique
  - K plus proches voisins (KNN)
  - Random Forest
  - Machine Ã  vecteurs de support (SVM)
  - Gradient Boosting
  - AdaBoost
  - Arbre de dÃ©cision
  - Naive Bayes

- MÃ©triques d'Ã©valuation complÃ¨tes :
  - PrÃ©cision (Accuracy)
  - PrÃ©cision (Precision)
  - Rappel (Recall)
  - ROC AUC
  - Scores de validation croisÃ©e
  - Temps d'entraÃ®nement

- IntÃ©gration MLflow pour le suivi des expÃ©riences
- Tableau de bord Streamlit interactif
- Suite de tests complÃ¨te avec unittest et pytest

## ğŸ› ï¸ Installation

1. Cloner le dÃ©pÃ´t :
```bash
git clone <url-du-dÃ©pÃ´t>
cd breast-cancer-model-comparison
```

2. CrÃ©er et activer un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Sous Windows : venv\Scripts\activate
```

3. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### EntraÃ®nement des ModÃ¨les
Pour entraÃ®ner tous les modÃ¨les et enregistrer les rÃ©sultats dans MLflow :
```bash
python models/models_training.py
```

### ExÃ©cution des Tests
Lancer la suite de tests :
```bash
python -m pytest tests/test.py
```

### Lancement du Tableau de Bord
DÃ©marrer le tableau de bord Streamlit :
```bash
streamlit run dashboard/run_dashboard.py
```

## ğŸ“Š FonctionnalitÃ©s du Tableau de Bord
Le tableau de bord Streamlit propose :
- Comparaison interactive des performances des modÃ¨les
- Graphiques radar pour la visualisation des mÃ©triques
- Comparaison des temps d'entraÃ®nement
- Analyse de la prÃ©cision par validation croisÃ©e
- Affichage des mÃ©triques clÃ©s
- Tableau de comparaison dÃ©taillÃ© avec mise en Ã©vidence

![Tableau de comparaison](assets/images/mÃ©triques.png)

![Exploration interactive](assets/images/exploration.png)

## ğŸ§ª Tests
Le projet inclut des tests complets :
- Validation du chargement et prÃ©traitement des donnÃ©es
- VÃ©rification de la sÃ©paration train/test
- Test des configurations des modÃ¨les
- Validation du logging MLflow
- Analyse de l'importance des caractÃ©ristiques
- VÃ©rifications de la prÃ©vention des fuites de donnÃ©es

## ğŸ“ˆ IntÃ©gration MLflow
Le projet utilise MLflow pour suivre :
- Les paramÃ¨tres des modÃ¨les
- Les mÃ©triques de performance
- L'importance des caractÃ©ristiques
- Les artefacts des modÃ¨les
- Le temps d'entraÃ®nement
- Les rÃ©sultats de validation croisÃ©e

## ğŸ”§ ParamÃ¨tres des ModÃ¨les
Chaque modÃ¨le est configurÃ© avec un ensemble spÃ©cifique d'hyperparamÃ¨tres pour la recherche sur grille :

### RÃ©gression Logistique
- C : [0.1, 1.0, 10.0]
- max_iter : [200]
- solver : ['lbfgs', 'liblinear']

### Random Forest
- n_estimators : [100, 200]
- max_depth : [10, 20, None]
- min_samples_split : [2, 5]

(Et similaire pour les autres modÃ¨les - voir models_training.py pour les configurations complÃ¨tes)

## ğŸ“ Traitement des DonnÃ©es
- Mise Ã  l'Ã©chelle automatique des donnÃ©es avec StandardScaler
- SÃ©paration train/test stratifiÃ©e (80/20)
- Analyse de l'importance des caractÃ©ristiques pour les modÃ¨les applicables

## âš™ï¸ PrÃ©requis
- Python 3.8+
- scikit-learn
- MLflow
- Streamlit
- Plotly
- Pandas
- NumPy
- Pytest